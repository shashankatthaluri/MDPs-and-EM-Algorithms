{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tEjqZCuOHJ9O"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the states and actions\n",
        "states = ['start', 'move_left', 'move_right', 'pick_up_trash', 'recycle', 'end']\n",
        "actions = ['left', 'right', 'pick_up', 'recycle', 'wait']\n",
        "\n",
        "# Define the transition probabilities for each action and state\n",
        "def transition_probs(state, action):\n",
        "    if state == 'start' and action == 'wait':\n",
        "        return {'move_left': 1.0}\n",
        "    elif state == 'start' and action == 'pick_up':\n",
        "        return {'pick_up_trash': 1.0}\n",
        "    elif state == 'move_left' and action == 'wait':\n",
        "        return {'move_left': 1.0}\n",
        "    elif state == 'move_left' and action == 'right':\n",
        "        return {'start': 1.0}\n",
        "    elif state == 'move_left' and action == 'pick_up':\n",
        "        return {'pick_up_trash': 1.0}\n",
        "    elif state == 'move_right' and action == 'wait':\n",
        "        return {'move_right': 1.0}\n",
        "    elif state == 'move_right' and action == 'left':\n",
        "        return {'start': 1.0}\n",
        "    elif state == 'move_right' and action == 'pick_up':\n",
        "        return {'pick_up_trash': 1.0}\n",
        "    elif state == 'pick_up_trash' and action == 'wait':\n",
        "        return {'recycle': 1.0}\n",
        "    elif state == 'recycle' and action == 'wait':\n",
        "        return {'end': 1.0}\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Define the reward for each state\n",
        "def reward(state):\n",
        "    if state == 'pick_up_trash':\n",
        "        return 10.0\n",
        "    elif state == 'recycle':\n",
        "        return 20.0\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "# Define the discount factor\n",
        "gamma = 0.9\n",
        "\n",
        "# Define the value iteration algorithm\n",
        "def value_iteration():\n",
        "    # Initialize the value of each state to 0\n",
        "    V = {s: 0 for s in states}\n",
        "    \n",
        "    # Iterate until convergence\n",
        "    while True:\n",
        "        # Initialize the change in value to 0\n",
        "        delta = 0\n",
        "        \n",
        "        # Update the value of each state\n",
        "        for s in states:\n",
        "            v = V[s]\n",
        "            \n",
        "            # Calculate the maximum expected value over all actions\n",
        "            max_v = -float('inf')\n",
        "            for a in actions:\n",
        "                tp = transition_probs(s, a)\n",
        "                if tp is not None:\n",
        "                    expected_v = 0\n",
        "                    for s_ in tp:\n",
        "                        r = reward(s_)\n",
        "                        expected_v += tp[s_] * (r + gamma * V[s_])\n",
        "                    max_v = max(max_v, expected_v)\n",
        "            \n",
        "            # Update the value of the state\n",
        "            V[s] = max_v\n",
        "            \n",
        "            # Update the change in value\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        \n",
        "        # Check for convergence\n",
        "        if delta < 1e-6:\n",
        "            break\n",
        "    \n",
        "    # Return the optimal policy \n",
        "    policy = {}\n",
        "    for s in states:\n",
        "      policy[s] = max(actions, key=lambda a: expected_returns(s, a))\n",
        "\n",
        "    # Print the optimal policy\n",
        "    print(\"Optimal Policy:\")\n",
        "    for s in states:\n",
        "      print(f\"{s}: {policy[s]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CXEdV6PzIWZJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}